{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ab7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "# Load all necessary libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm().pandas()\n",
    "\n",
    "#import torch\n",
    "\n",
    "import sys, getopt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "# forecast monthly births with xgboost\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "#from matplotlib import pyplot\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.cluster import OPTICS\n",
    "import numpy as np; \n",
    "# Data manipulation\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plots\n",
    "# ==============================================================================\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#%matplotlib inline\n",
    "#from statsmodels.graphics.tsaplots import plot_acf\n",
    "#from statsmodels.graphics.tsaplots import plot_pacf\n",
    "#plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Modelado y Forecasting\n",
    "# ==============================================================================\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.ForecasterAutoregMultiOutput import ForecasterAutoregMultiOutput\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.cluster import OPTICS\n",
    "import pickle\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "#from pandarallel import pandarallel\n",
    "#pandarallel.initialize(progress_bar=True)\n",
    "from array import *\n",
    "import schedule\n",
    "import time\n",
    "import psycopg2 as pg\n",
    "import pandas.io.sql as psql\n",
    "\n",
    "from h3 import h3\n",
    "import psycopg2 as pg\n",
    "import pandas.io.sql as psql\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "#class Data # 2. Class which describes a single flower measurements\n",
    "class Predict:\n",
    "    # 6. Class constructor, loads the dataset and loads the model\n",
    "    #    if exists. If not, calls the _train_model method and \n",
    "    #    saves the model\n",
    "    def __init__(self):\n",
    "        #self.df = datasetttt\n",
    "        self.model_fname_ = 'model.pkl'\n",
    "    def read_data(self):\n",
    "        global ar\n",
    "        ar = []\n",
    "        global pred\n",
    "        pred = []\n",
    "        global datasetttt\n",
    "        connection = pg.connect(\"host=gw-sand-toyou.net.amhub.org dbname=amdelivery_sandbox user=a.zabolotskii password=r7LJ3WSR5PAYLYV3 sslmode=require\")\n",
    "        dataframedelivery = psql.read_sql('SELECT * FROM delivery_order LIMIT 20000', connection)\n",
    "\n",
    "        connection2 = pg.connect(\"host=gw-sand-toyou.net.amhub.org dbname=amdelivery_sandbox user=a.zabolotskii password=r7LJ3WSR5PAYLYV3 sslmode=require\")\n",
    "        dataframelocation = psql.read_sql('SELECT * FROM location  LIMIT 1000000', connection2)\n",
    "\n",
    "        merged_2 = dataframedelivery.merge(dataframelocation, how='inner', left_on=[\"pick_up_location\"], right_on=[\"id\"])\n",
    "        cols = ['lat','lon','creation_date']\n",
    "        data = merged_2[cols]\n",
    "        df2 = data\n",
    "        df2['creation_date'] = data['creation_date'].astype('datetime64[h]')\n",
    "        df2['orders'] = 1\n",
    "        df2= df2.groupby([\"lon\",\"lat\", \"creation_date\"], as_index=False)[\"orders\"].count()\n",
    "        df2 = df2.sort_values(by = ['creation_date'], ascending = [False])\n",
    "        h3_level = 9\n",
    " \n",
    "        def lat_lng_to_h3(row):\n",
    "            return h3.geo_to_h3(\n",
    "                row.lat, row.lon, h3_level)\n",
    " \n",
    "        orders = df2.apply(lat_lng_to_h3, axis=1)\n",
    "        df2['fid'] = orders\n",
    "        df2 = df2.rename(columns={\"creation_date\": \"time\"})\n",
    "        table = pd.pivot_table(df2, values='orders', index=['time'],columns=['fid'])\n",
    "        table = table.fillna(0)\n",
    "        table = table.sort_values(by = ['time'], ascending = [False])\n",
    "   \n",
    "        datasetttt = table[0:1100]\n",
    "        datasetttt = datasetttt.iloc[:,:10]\n",
    "        return datasetttt\n",
    "    \n",
    "    def fitdata(self,datasetttt):\n",
    "        end_validation = 951\n",
    "        end_train = 800\n",
    "        for index in range(datasetttt.shape[1]):\n",
    "            columnSeriesObj = datasetttt.iloc[:, index]\n",
    "            columnSeriesObj = pd.Series(list(columnSeriesObj))\n",
    "            forecaster = ForecasterAutoregMultiOutput(\n",
    "                regressor = RandomForestRegressor(max_depth=14),lags = 20,steps = 24)\n",
    "            columnSeriesObj1 = columnSeriesObj[48:1024]\n",
    "            columnSeriesObj2 = columnSeriesObj[24:1000]\n",
    "            i = columnSeriesObj1.index\n",
    "            columnSeriesObj2.index = i\n",
    "    \n",
    "            param_grid = {'n_estimators': [100, 500],'max_depth': [4, 6]}\n",
    "\n",
    "            lags_grid = [[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24], [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]]\n",
    "            results_grid = grid_search_forecaster(\n",
    "                        forecaster  = forecaster,\n",
    "                        y           = pd.Series(list(columnSeriesObj2)),#data[:,index],table11 = pd.Series(list(table1))   \n",
    "                        exog        = pd.Series(list(columnSeriesObj1)),#data[:,index],\n",
    "                        param_grid  = param_grid,\n",
    "                        lags_grid   = lags_grid,\n",
    "                        steps       = 24,\n",
    "                        metric      = 'mean_absolute_error',\n",
    "                        refit       = False,\n",
    "                        initial_train_size = 900,\n",
    "                        return_best = True,\n",
    "                        verbose     = False\n",
    "                  )\n",
    "            return forecaster\n",
    "        \n",
    "    def predictions(self):\n",
    "        end_train = 800\n",
    "        end_validation = 951\n",
    "        for index in range(datasetttt.shape[1]):\n",
    "            columnSeriesObj = datasetttt.iloc[:, index]\n",
    "            columnSeriesObj = pd.Series(list(columnSeriesObj))\n",
    "            columnSeriesObj1 = columnSeriesObj[48:1024]\n",
    "            columnSeriesObj2 = columnSeriesObj[0:976]\n",
    "            i = columnSeriesObj1.index\n",
    "            columnSeriesObj2.index = i\n",
    "    #columnSeriesObj3 = columnSeriesObj[24:1000]\n",
    "            columnSeriesObj3 = columnSeriesObj[0:976]\n",
    "        #forecaster = fitdata(datasetttt)\n",
    "            global forecaster\n",
    "            forecaster = ForecasterAutoregMultiOutput(\n",
    "                regressor = RandomForestRegressor(max_depth=14),\n",
    "                lags = 20,\n",
    "                steps = 24\n",
    "                )\n",
    "            metric, predictions = backtesting_forecaster(\n",
    "                            forecaster = forecaster,\n",
    "                            y          = pd.Series(list(columnSeriesObj2)),\n",
    "                            exog       = pd.Series(list(columnSeriesObj1)),\n",
    "                            initial_train_size = len(columnSeriesObj[:end_validation]),\n",
    "                            steps      = 24,\n",
    "                            metric     = 'mean_absolute_error',\n",
    "                            refit      = False,\n",
    "                            verbose    = False)\n",
    "            ar.append(predictions.copy())\n",
    "            print(ar)       \n",
    "    def save_data(self):\n",
    "        forecasterd = self.fitdata(datasetttt)\n",
    "        PIK = \"models.pckl\"\n",
    "        pred.append(forecasterd)\n",
    "        with open(\"models.pckl\", \"wb\") as f:\n",
    "            for forecaster in pred:\n",
    "                pickle.dump(forecasterd, f)\n",
    "            \n",
    "        with open(PIK, \"rb\") as f:\n",
    "            print(pickle.load(f))\n",
    "        arr = np.array(ar)\n",
    "        arr = arr.reshape(25,10)\n",
    "        arr = pd.DataFrame(arr,index=datasetttt[0:25].index,columns=datasetttt[0:10].columns )    \n",
    "        print(arr)           \n",
    "            \n",
    "    def main(argv):\n",
    "   # if torch.cuda.is_available():\n",
    "   #     dev = \"cuda:0\"\n",
    "        \n",
    "   # else:\n",
    "   #     dev = \"cpu\"\n",
    "   #     device = torch.device(dev)\n",
    "    #    torch.cuda.set_device(-1)\n",
    "        if __name__ == \"__main__\":\n",
    "            \n",
    "            main(sys.argv[1:])\n",
    "p = Predict()             \n",
    "schedule.every(24).hours.do(p.read_data)\n",
    "datasetttt = p.read_data()     \n",
    "schedule.every().monday.do(p.fitdata)  \n",
    "p.fitdata(datasetttt)\n",
    "schedule.every(24).hours.do(p.predictions) \n",
    "p.predictions()\n",
    "schedule.every(24).hours.do(p.save_data)\n",
    "p.save_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
